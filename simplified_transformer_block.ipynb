{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Detour) Simplified attention block\n",
    "\n",
    "Paper: [Simplifying Transformer Blocks -- He et al.](https://arxiv.org/abs/2311.01906), removes the need for skip connections (which increases training efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedMQA(nn.Module):\n",
    "    \"\"\"Simplified Multi-Query Attention implementation based on \"Simplifying Transformer Blocks\" (He & Hofmann, 2024)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_in: int, d_out: int, context_length: int, dropout: float, num_heads: int, qkv_bias: bool = False, split_into_chunks: bool = True, zero_init_query: bool = False, is_casual: bool = True, do_dropout: bool = False):\n",
    "        \"\"\"Initializes the Simplified Multi-Query Attention module.\n",
    "    \n",
    "        Args:\n",
    "            d_in (int): Input dimension\n",
    "            d_out (int): Output dimension\n",
    "            context_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout probability\n",
    "            num_heads (int): Number of attention heads\n",
    "            qkv_bias (bool): Whether to include bias in query/key/value projections\n",
    "            split_into_chunks (bool): If True, splits input directly into heads for values (used in all layers except first).\n",
    "                                    If False, projects input to head_dim using W_v (used in first layer only).\n",
    "            zero_init_query (bool): Whether to initialize query weights to zero (as in original paper)\n",
    "            is_casual (bool): Whether to use causal masking (for autoregressive models)\n",
    "            do_dropout (bool): Whether to apply dropout to attention weights (recommended for fine-tuning)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # For First layer, project input from d_in to head_dim, otherwise, split into h chunks\n",
    "        self.split_into_chunks = split_into_chunks\n",
    "        self.is_casual = is_casual\n",
    "        self.do_dropout = do_dropout\n",
    "        \n",
    "        # Query projects to full d_out dimension (will be split into heads)\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Key and Value project only to head_dim (shared across heads)\n",
    "        self.W_k = nn.Linear(d_in, self.head_dim, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, self.head_dim, bias=qkv_bias) if not self.split_into_chunks else None\n",
    "        \n",
    "        # Initialize W_q to zero if nessesary (as mentioned in original paper)\n",
    "        if zero_init_query:\n",
    "            nn.init.zeros_(self.W_q.weight)\n",
    "            if self.W_q.bias is not None:\n",
    "                nn.init.zeros_(self.W_q.bias)\n",
    "\n",
    "        # Initialize learnable parameters per head\n",
    "        self.alpha = nn.Parameter(torch.ones(num_heads))  # shape: (num_heads,)\n",
    "        self.beta = nn.Parameter(torch.ones(num_heads))   # shape: (num_heads,)\n",
    "        self.gamma = nn.Parameter(torch.ones(num_heads))  # shape: (num_heads,)\n",
    "        \n",
    "        # No outwards projection as the outwards projection layer often approximates the identity matrix (according to the paper)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # Mask for future tokens (since tokens are casual, and future tokens should not influence past tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Project queries to full dimension and reshape\n",
    "        queries = self.W_q(x).view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Project keys and values to single head dimension\n",
    "        keys = self.W_k(x)    # shape: (batch, num_tokens, head_dim)\n",
    "        if self.split_into_chunks:\n",
    "            values = x.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "            values = values.transpose(1, 2)  # (batch, num_heads, num_tokens, head_dim)\n",
    "        else:\n",
    "            values = self.W_v(x)  # shape: (batch, num_tokens, head_dim)\n",
    "        \n",
    "        # Transpose queries for attention computation\n",
    "        queries = queries.transpose(1, 2)  # (batch, num_heads, num_tokens, head_dim)\n",
    "        \n",
    "        # Reshape alpha, beta, gamma for broadcasting\n",
    "        # Add dimensions for batch and sequence length\n",
    "        alpha = self.alpha.view(1, self.num_heads, 1, 1)  # shape: (1, num_heads, 1, 1)\n",
    "        beta = self.beta.view(1, self.num_heads, 1, 1)    # shape: (1, num_heads, 1, 1)\n",
    "        gamma = self.gamma.view(1, self.num_heads, 1, 1)  # shape: (1, num_heads, 1, 1)\n",
    "        \n",
    "        # Creating the Identity component\n",
    "        identity = torch.eye(num_tokens, device=x.device)[None, None, :, :]\n",
    "        \n",
    "        # Centering matrix \n",
    "        C = torch.ones(num_tokens, num_tokens, device=x.device) / num_tokens\n",
    "        C = C[None, None, :, :]\n",
    "        if self.is_casual:\n",
    "            C.masked_fill(mask_bool, 0)\n",
    "            C = C / (C.sum(dim=-1, keepdim=True) + 1e-8) # Renormalized to 1 after (\"casual\") masking\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # keys/values are broadcast across num_heads dimension\n",
    "        attn_scores = queries @ keys.unsqueeze(1).transpose(-2, -1)\n",
    "        if self.is_casual:   \n",
    "            attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.head_dim**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Dropout attention weights if user decides to do so (like in finetuning during original paper)\n",
    "        if self.do_dropout:\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Shaped attention\n",
    "        shaped_attn = (alpha * identity + beta * attn_weights - gamma * C)\n",
    "        \n",
    "        if self.split_into_chunks:\n",
    "            context_vec = (shaped_attn @ values).transpose(1, 2) # Values already has heads dimension\n",
    "        else:\n",
    "            context_vec = (shaped_attn @ values.unsqueeze(1)).transpose(1, 2) # Values needs broadcasting across heads\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        context_vec = context_vec.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedTransformerBlock(nn.Module):\n",
    "    \"\"\"Simplified Transformer Block based on \"Simplifying Transformer Blocks\" (He & Hofmann, 2024)\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,  # Usually 4*d_model\n",
    "                 context_length: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_idx: int = 0,\n",
    "                 split_into_chunks: Optional[bool] = None,\n",
    "                 beta_ff: float = 0.1,\n",
    "                 activation=F.relu):\n",
    "        \"\"\"Initializes the Simplified Transformer Block.\n",
    "    \n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            mlp_dim (int): Dimension of MLP hidden layer (usually 4*d_model)\n",
    "            context_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout probability\n",
    "            layer_idx (int): Index of this layer in the stack\n",
    "            split_into_chunks (Optional[bool]): Whether to split values into chunks or use projection.\n",
    "                                            If None, defaults to True for all layers except first.\n",
    "            beta_ff (float): Initial scale for MLP output (paper suggests 0.1 for depth 18)\n",
    "            activation: Activation function to use in MLP\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        split_into_chunks = split_into_chunks if split_into_chunks else layer_idx != 0\n",
    "        \n",
    "        # Attention path\n",
    "        self.attention = SimplifiedMQA(\n",
    "            d_in=d_model,\n",
    "            d_out=d_model,\n",
    "            context_length=context_length,\n",
    "            dropout=dropout,\n",
    "            num_heads=num_heads,\n",
    "            split_into_chunks=split_into_chunks  # Only first layer uses value projection\n",
    "        )\n",
    "        \n",
    "        # MLP path\n",
    "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
    "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize MLP residual scale as mentioned in paper\n",
    "        self.beta_ff = nn.Parameter(torch.tensor(beta_ff))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Parallel processing of attention and MLP paths\n",
    "        attn_out = self.attention(x)\n",
    "        mlp_out = self.mlp_out(self.activation(self.mlp_in(x)))\n",
    "        \n",
    "        # Combine paths (with optional MLP scaling)\n",
    "        return attn_out + self.beta_ff * mlp_out\n",
    "\n",
    "class SimplifiedTransformer(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SimplifiedTransformerBlock(layer_idx=i, **block_args)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
