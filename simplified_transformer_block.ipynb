{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Detour) Simplified attention block\n",
    "\n",
    "Paper: [Simplifying Transformer Blocks -- He et al.](https://arxiv.org/abs/2311.01906), removes the need for skip connections (which increases training efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedMQA(nn.Module):\n",
    "    \"\"\"Simplified Multi-Query Attention implementation based on \"Simplifying Transformer Blocks\" (He & Hofmann, 2024)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_in: int, d_out: int, context_length: int, dropout: float, num_heads: int, qkv_bias: bool = False, split_into_chunks: bool = True, zero_init_query: bool = False, is_casual: bool = True, do_dropout: bool = False, **kwargs):\n",
    "        \"\"\"Initializes the Simplified Multi-Query Attention module.\n",
    "    \n",
    "        Args:\n",
    "            d_in (int): Input dimension\n",
    "            d_out (int): Output dimension\n",
    "            context_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout probability\n",
    "            num_heads (int): Number of attention heads\n",
    "            qkv_bias (bool): Whether to include bias in query/key/value projections\n",
    "            split_into_chunks (bool): If True, splits input directly into heads for values (used in all layers except first).\n",
    "                                    If False, projects input to head_dim using W_v (used in first layer only).\n",
    "            zero_init_query (bool): Whether to initialize query weights to zero (as in original paper)\n",
    "            is_casual (bool): Whether to use causal masking (for autoregressive models)\n",
    "            do_dropout (bool): Whether to apply dropout to attention weights (recommended for fine-tuning)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # For First layer, project input from d_in to head_dim, otherwise, split into h chunks\n",
    "        self.split_into_chunks = split_into_chunks\n",
    "        self.is_casual = is_casual\n",
    "        self.do_dropout = do_dropout\n",
    "        \n",
    "        # Query projects to full d_out dimension (will be split into heads)\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Key and Value project only to head_dim (shared across heads)\n",
    "        self.W_k = nn.Linear(d_in, self.head_dim, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, self.head_dim, bias=qkv_bias) if not self.split_into_chunks else None\n",
    "        \n",
    "        # Initialize W_q to zero if nessesary (as mentioned in original paper)\n",
    "        if zero_init_query:\n",
    "            nn.init.zeros_(self.W_q.weight)\n",
    "            if self.W_q.bias is not None:\n",
    "                nn.init.zeros_(self.W_q.bias)\n",
    "\n",
    "        # Initialize learnable parameters per head\n",
    "        self.alpha = nn.Parameter(torch.ones(num_heads))  # shape: (num_heads,)\n",
    "        self.beta = nn.Parameter(torch.ones(num_heads))   # shape: (num_heads,)\n",
    "        self.gamma = nn.Parameter(torch.ones(num_heads))  # shape: (num_heads,)\n",
    "        \n",
    "        # No outwards projection as the outwards projection layer often approximates the identity matrix (according to the paper)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # Mask for future tokens (since tokens are casual, and future tokens should not influence past tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Attention mask (to mask padding tokens) to prevent unnessesary attention between normal tokens and padding tokens\n",
    "        if attention_mask is not None:\n",
    "            padding_mask = attention_mask[:, None, None, :] == 0\n",
    "            mask_bool = mask_bool | padding_mask\n",
    "        \n",
    "        # Project queries to full dimension and reshape\n",
    "        queries = self.W_q(x).view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Project keys and values to single head dimension\n",
    "        keys = self.W_k(x)    # shape: (batch, num_tokens, head_dim)\n",
    "        if self.split_into_chunks:\n",
    "            values = x.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "            values = values.transpose(1, 2)  # (batch, num_heads, num_tokens, head_dim)\n",
    "        else:\n",
    "            values = self.W_v(x)  # shape: (batch, num_tokens, head_dim)\n",
    "        \n",
    "        # Transpose queries for attention computation\n",
    "        queries = queries.transpose(1, 2)  # (batch, num_heads, num_tokens, head_dim)\n",
    "        \n",
    "        # Reshape alpha, beta, gamma for broadcasting\n",
    "        # Add dimensions for batch and sequence length\n",
    "        alpha = self.alpha.view(1, self.num_heads, 1, 1)  # shape: (1, num_heads, 1, 1)\n",
    "        beta = self.beta.view(1, self.num_heads, 1, 1)    # shape: (1, num_heads, 1, 1)\n",
    "        gamma = self.gamma.view(1, self.num_heads, 1, 1)  # shape: (1, num_heads, 1, 1)\n",
    "        \n",
    "        # Creating the Identity component\n",
    "        identity = torch.eye(num_tokens, device=x.device)[None, None, :, :]\n",
    "        \n",
    "        # Centering matrix \n",
    "        C = torch.ones(num_tokens, num_tokens, device=x.device) / num_tokens\n",
    "        C = C[None, None, :, :]\n",
    "        if self.is_casual:\n",
    "            C.masked_fill(mask_bool, 0)\n",
    "            C = C / (C.sum(dim=-1, keepdim=True) + 1e-8) # Renormalized to 1 after (\"casual\") masking\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # keys/values are broadcast across num_heads dimension\n",
    "        attn_scores = queries @ keys.unsqueeze(1).transpose(-2, -1)\n",
    "        if self.is_casual:   \n",
    "            attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.head_dim**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        # Dropout attention weights if user decides to do so (like in finetuning during original paper)\n",
    "        if self.do_dropout:\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Shaped attention\n",
    "        shaped_attn = (alpha * identity + beta * attn_weights - gamma * C)\n",
    "        \n",
    "        if self.split_into_chunks:\n",
    "            context_vec = (shaped_attn @ values).transpose(1, 2) # Values already has heads dimension\n",
    "        else:\n",
    "            context_vec = (shaped_attn @ values.unsqueeze(1)).transpose(1, 2) # Values needs broadcasting across heads\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        context_vec = context_vec.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedTransformerBlock(nn.Module):\n",
    "    \"\"\"Simplified Transformer Block based on \"Simplifying Transformer Blocks\" (He & Hofmann, 2024)\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,  # Usually 4*d_model\n",
    "                 context_length: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_idx: int = 0,\n",
    "                 split_into_chunks: Optional[bool] = None,\n",
    "                 beta_ff: float = 0.1,\n",
    "                 activation=F.relu,\n",
    "                 use_norm: bool = True,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes the Simplified Transformer Block.\n",
    "    \n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            mlp_dim (int): Dimension of MLP hidden layer (usually 4*d_model)\n",
    "            context_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout probability\n",
    "            layer_idx (int): Index of this layer in the stack\n",
    "            split_into_chunks (Optional[bool]): Whether to split values into chunks or use projection.\n",
    "                                            If None, defaults to True for all layers except first.\n",
    "            beta_ff (float): Initial scale for MLP output (paper suggests 0.1 for depth 18)\n",
    "            activation (Callable[Tensor, Tensor]): Activation function to use in MLP\n",
    "            use_norm (boolean): Wheather or not to use Layernorm for the inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add normalization layer\n",
    "        self.norm = nn.LayerNorm(d_model) if use_norm else nn.Identity()\n",
    "        \n",
    "        # Split into chunks according to the \n",
    "        split_into_chunks = split_into_chunks if (split_into_chunks is not None) else layer_idx != 0\n",
    "        \n",
    "        # Attention path\n",
    "        self.attention = SimplifiedMQA(\n",
    "            d_in=d_model,\n",
    "            d_out=d_model,\n",
    "            context_length=context_length,\n",
    "            dropout=dropout,\n",
    "            num_heads=num_heads,\n",
    "            split_into_chunks=split_into_chunks  # Only first layer uses value projection\n",
    "        )\n",
    "        \n",
    "        # MLP path\n",
    "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
    "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize MLP residual scale as mentioned in paper\n",
    "        self.beta_ff = nn.Parameter(torch.tensor(beta_ff))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        # Apply normalization before parallel paths\n",
    "        normed_x = self.norm(x)\n",
    "        \n",
    "        # Parallel processing on normalized input\n",
    "        attn_out = self.attention(normed_x, attention_mask)\n",
    "        mlp_out = self.mlp_out(self.activation(self.mlp_in(normed_x)))\n",
    "        \n",
    "        # Combine paths (with optional MLP scaling)\n",
    "        return attn_out + self.beta_ff * mlp_out\n",
    "\n",
    "class SimplifiedTransformer(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SimplifiedTransformerBlock(layer_idx=i, **block_args)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(block_args['d_model']) if block_args['use_norm'] else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.final_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initial loss: 7.081815242767334\n",
      "Step 2, Loss: 7.036366939544678\n",
      "Step 4, Loss: 6.945870876312256\n",
      "Step 6, Loss: 6.855895042419434\n",
      "Step 8, Loss: 6.766436576843262\n",
      "Step 10, Loss: 6.677464485168457\n"
     ]
    }
   ],
   "source": [
    "# Create a small synthetic dataset to verify the architecture works\n",
    "batch_size = 4\n",
    "seq_len = 128\n",
    "d_model = 256\n",
    "vocab_size = 1000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, **transformer_args):\n",
    "        super().__init__()\n",
    "        transformer_args = transformer_args | {\"d_model\": d_model, \"vocab_size\": vocab_size}\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(transformer_args[\"context_length\"], d_model)\n",
    "        self.transformer = SimplifiedTransformer(**transformer_args)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)  # Project back to vocabulary\n",
    "        \n",
    "        self.ctx_len = transformer_args[\"context_length\"]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n",
    "        \n",
    "        x = self.embedding(x) + self.position_embedding(positions)\n",
    "        x = self.transformer(x)\n",
    "        return self.lm_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple testing (on whether we implemented the architecture correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    mlp_dim=d_model * 4,\n",
    "    context_length=seq_len,\n",
    "    dropout=0.1,\n",
    "    use_norm=True\n",
    ").to(device)\n",
    "\n",
    "# Create synthetic data\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device) # Input tokens\n",
    "y = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # Target tokens\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "# Verify backprop works\n",
    "loss.backward()\n",
    "\n",
    "# Optional: Add small training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Initial loss:\", loss.item())\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f\"Step {i+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
